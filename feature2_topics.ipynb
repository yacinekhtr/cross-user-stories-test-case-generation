{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from docx import Document\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.parsing.preprocessing import preprocess_string, STOPWORDS\n",
    "from difflib import ndiff, unified_diff\n",
    "\n",
    "# Chargement du modèle spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Votre code principal ici\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YacineKHITER\\cross-user-stories-test-case-generation\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "  File \"C:\\Users\\YacineKHITER\\AppData\\Local\\Temp\\ipykernel_40384\\1743858135.py\", line 2, in <module>\n",
      "    nlp = spacy.load('en_core_web_sm')\n",
      "          ^^^^^\n",
      "NameError: name 'spacy' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YacineKHITER\\cross-user-stories-test-case-generation\\myenv\\Lib\\site-packages\\pygments\\styles\\__init__.py\", line 45, in get_style_by_name\n",
      "ModuleNotFoundError: No module named 'pygments.styles.default'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YacineKHITER\\cross-user-stories-test-case-generation\\myenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2168, in showtraceback\n",
      "  File \"c:\\Users\\YacineKHITER\\cross-user-stories-test-case-generation\\myenv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1454, in structured_traceback\n",
      "  File \"c:\\Users\\YacineKHITER\\cross-user-stories-test-case-generation\\myenv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1345, in structured_traceback\n",
      "  File \"c:\\Users\\YacineKHITER\\cross-user-stories-test-case-generation\\myenv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1192, in structured_traceback\n",
      "  File \"c:\\Users\\YacineKHITER\\cross-user-stories-test-case-generation\\myenv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "  File \"c:\\Users\\YacineKHITER\\cross-user-stories-test-case-generation\\myenv\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1133, in get_records\n",
      "  File \"c:\\Users\\YacineKHITER\\cross-user-stories-test-case-generation\\myenv\\Lib\\site-packages\\pygments\\styles\\__init__.py\", line 47, in get_style_by_name\n",
      "pygments.util.ClassNotFound: Could not find style module 'pygments.styles.default', though it should be builtin.\n"
     ]
    }
   ],
   "source": [
    "# Upload english language model of spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    full_text = []\n",
    "\n",
    "    # Extract text from paragraph \n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    \n",
    "    \n",
    "    # Extract text from tables\n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            row_text = [cell.text for cell in row.cells]\n",
    "            full_text.append(' '.join(row_text))\n",
    "    \n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def get_user_stories_from_folder(folder_path):\n",
    "    user_stories = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".docx\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            text = extract_text_from_docx(file_path)\n",
    "            user_stories.append(text)\n",
    "    return user_stories\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in nlp(text):\n",
    "        if token.text.lower() not in STOPWORDS and not token.is_punct and not token.is_space:\n",
    "            result.append(token.lemma_.lower())\n",
    "    return result\n",
    "\n",
    "def identify_topics(user_stories, num_topics=3):\n",
    "    # Preprocess user stories\n",
    "    processed_stories = [preprocess(story) for story in user_stories]\n",
    "    \n",
    "    # Create dictionnary and the corpus \n",
    "    dictionary = corpora.Dictionary(processed_stories)\n",
    "    corpus = [dictionary.doc2bow(story) for story in processed_stories]\n",
    "    \n",
    "    \n",
    "    # Use LDA\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "    \n",
    "    return lda_model\n",
    "\n",
    "\n",
    "# Path to folder contening  Word files\n",
    "folder_path = \"C:\\\\Users\\\\YacineKHITER\\\\Documents\\\\User stories\\\\Paquet 1\"\n",
    "\n",
    "# Extraction of user stories \n",
    "user_stories = get_user_stories_from_folder(folder_path)\n",
    "\n",
    "\n",
    "# Identify Topics\n",
    "lda_model = identify_topics(user_stories)\n",
    "\n",
    "# Display the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f'Topic {idx}: {topic}')\n",
    "\n",
    "\n",
    "# Analyse a specific user story \n",
    "def get_story_topics(text, lda_model, dictionary):\n",
    "    bow = dictionary.doc2bow(preprocess(text))\n",
    "    return lda_model[bow]\n",
    "\n",
    "\n",
    "# Display Topics from a specific user story \n",
    "for story in user_stories:\n",
    "    print(f'\\nUser Story: {story[:100]}...')  # Affiche les 100 premiers caractères pour context\n",
    "    topics = get_story_topics(story, lda_model, lda_model.id2word)\n",
    "    print('Topics:', topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.048*\"payment\" + 0.048*\"user\" + 0.046*\"order\" + 0.038*\"able\" + 0.032*\"ibm\" + 0.027*\"detail\" + 0.027*\"number\" + 0.022*\"receive\" + 0.019*\"information\" + 0.019*\"date\"\n",
      "Topic 1: 0.063*\"invoice\" + 0.032*\"user\" + 0.032*\"able\" + 0.032*\"order\" + 0.026*\"detail\" + 0.026*\"information\" + 0.026*\"number\" + 0.026*\"billing\" + 0.020*\"payment\" + 0.020*\"ibm\"\n",
      "Topic 2: 0.039*\"price\" + 0.039*\"option\" + 0.035*\"able\" + 0.030*\"user\" + 0.025*\"storage\" + 0.025*\"processor\" + 0.025*\"ram\" + 0.020*\"server\" + 0.020*\"capacity\" + 0.020*\"display\"\n",
      "\n",
      "User Story: \n",
      "ID UC-002\n",
      "Name IBM Order Validation\n",
      "Description 1. As a customer, I want my server order to be vali...\n",
      "Assigned Common Topic: 2\n",
      "Cluster: 0\n",
      "\n",
      "User Story: \n",
      "                   \n",
      "\n",
      "ID UC-001\n",
      "Name IBM Server Order\n",
      "Description 1. As a customer, I want to be abl...\n",
      "Assigned Common Topic: 2\n",
      "Cluster: 0\n",
      "\n",
      "User Story: \n",
      "ID UC-005\n",
      "Name Recording the Invoice Payment and Accounting\n",
      "Description 1. As a customer, I want my...\n",
      "Assigned Common Topic: 2\n",
      "Cluster: 0\n",
      "\n",
      "User Story: \n",
      "ID UC-003\n",
      "Name Sending the Advance Shipment Notification (ASN) by IBM\n",
      "Description 1. As a customer,...\n",
      "Assigned Common Topic: 2\n",
      "Cluster: 0\n",
      "\n",
      "User Story: \n",
      "ID UC-004\n",
      "Name Sending the Invoice by IBM\n",
      "Description 1. As a customer, I want to receive an invoic...\n",
      "Assigned Common Topic: 2\n",
      "Cluster: 0\n",
      "Coherence of the LDA model: 0.5906\n",
      "\n",
      "User Story: \n",
      "ID UC-002\n",
      "Name IBM Order Validation\n",
      "Description 1. As a customer, I want my server order to be vali...\n",
      "Topics: [(0, 0.9935367)]\n",
      "Highest Topic: (0, 0.9935367)\n",
      "\n",
      "User Story: \n",
      "                   \n",
      "\n",
      "ID UC-001\n",
      "Name IBM Server Order\n",
      "Description 1. As a customer, I want to be abl...\n",
      "Topics: [(2, 0.9954295)]\n",
      "Highest Topic: (2, 0.9954295)\n",
      "\n",
      "User Story: \n",
      "ID UC-005\n",
      "Name Recording the Invoice Payment and Accounting\n",
      "Description 1. As a customer, I want my...\n",
      "Topics: [(0, 0.9931041)]\n",
      "Highest Topic: (0, 0.9931041)\n",
      "\n",
      "User Story: \n",
      "ID UC-003\n",
      "Name Sending the Advance Shipment Notification (ASN) by IBM\n",
      "Description 1. As a customer,...\n",
      "Topics: [(0, 0.99344283)]\n",
      "Highest Topic: (0, 0.99344283)\n",
      "\n",
      "User Story: \n",
      "ID UC-004\n",
      "Name Sending the Invoice by IBM\n",
      "Description 1. As a customer, I want to receive an invoic...\n",
      "Topics: [(1, 0.99308944)]\n",
      "Highest Topic: (1, 0.99308944)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from docx import Document\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.parsing.preprocessing import preprocess_string, STOPWORDS\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Load spaCy English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    full_text = []\n",
    "\n",
    "    # Extract text from paragraphs\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    \n",
    "    # Extract text from tables\n",
    "    for table in doc.tables:\n",
    "        for row in table.rows:\n",
    "            row_text = [cell.text for cell in row.cells]\n",
    "            full_text.append(' '.join(row_text))\n",
    "    \n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def get_user_stories_from_folder(folder_path):\n",
    "    user_stories = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".docx\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            text = extract_text_from_docx(file_path)\n",
    "            user_stories.append(text)\n",
    "    return user_stories\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in nlp(text):\n",
    "        if token.text.lower() not in STOPWORDS and not token.is_punct and not token.is_space:\n",
    "            result.append(token.lemma_.lower())\n",
    "    return result\n",
    "\n",
    "def identify_topics(user_stories, num_topics=3, random_state=42):\n",
    "    # Preprocess user stories\n",
    "    processed_stories = [preprocess(story) for story in user_stories]\n",
    "    \n",
    "    # Create dictionary and the corpus\n",
    "    dictionary = corpora.Dictionary(processed_stories)\n",
    "    corpus = [dictionary.doc2bow(story) for story in processed_stories]\n",
    "    \n",
    "    # Use LDA with fixed random state\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=random_state)\n",
    "    \n",
    "    # Calculate coherence\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_stories, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    \n",
    "    return lda_model, dictionary, corpus, coherence_lda\n",
    "\n",
    "def cluster_documents(lda_model, corpus, num_clusters=1, random_state=42):\n",
    "    # Get document-topic distribution\n",
    "    doc_topics = [lda_model.get_document_topics(bow) for bow in corpus]\n",
    "    \n",
    "    # Convert to a matrix\n",
    "    doc_topic_matrix = np.zeros((len(doc_topics), lda_model.num_topics))\n",
    "    for i, doc in enumerate(doc_topics):\n",
    "        for topic_num, prob in doc:\n",
    "            doc_topic_matrix[i, topic_num] = prob\n",
    "    \n",
    "    # Cluster the documents with fixed random state\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=random_state).fit(doc_topic_matrix)\n",
    "    return kmeans.labels_\n",
    "\n",
    "def get_highest_coherence_topic(user_stories, lda_model, dictionary):\n",
    "    processed_stories = [preprocess(story) for story in user_stories]\n",
    "    coherence_values = []\n",
    "    \n",
    "    for i in range(lda_model.num_topics):\n",
    "        topics = [[word for word, prob in lda_model.show_topic(i, topn=10)]]\n",
    "        coherence_model = CoherenceModel(topics=topics, texts=processed_stories, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    highest_coherence_topic = np.argmax(coherence_values)\n",
    "    return highest_coherence_topic\n",
    "\n",
    "# Path to folder containing Word files\n",
    "folder_path = \"C:\\\\Users\\\\YacineKHITER\\\\Documents\\\\User stories\\\\Paquet 1\"\n",
    "\n",
    "# Extract user stories\n",
    "user_stories = get_user_stories_from_folder(folder_path)\n",
    "\n",
    "# Identify Topics\n",
    "lda_model, dictionary, corpus, coherence_lda = identify_topics(user_stories, random_state=42)\n",
    "\n",
    "# Cluster documents\n",
    "num_clusters = 1  # You can change this based on the number of expected clusters\n",
    "clusters = cluster_documents(lda_model, corpus, num_clusters=num_clusters, random_state=42)\n",
    "\n",
    "# Calculate topic coherence for each user story\n",
    "user_story_topics = []\n",
    "for story in user_stories:\n",
    "    topics = lda_model.get_document_topics(dictionary.doc2bow(preprocess(story)))\n",
    "    user_story_topics.append(topics)\n",
    "\n",
    "# Find the highest coherence topic\n",
    "highest_coherence_topic_value = 0\n",
    "highest_coherence_topic = None\n",
    "for i, story_topics in enumerate(user_story_topics):\n",
    "    for topic_num, topic_value in story_topics:\n",
    "        if topic_value > highest_coherence_topic_value:\n",
    "            highest_coherence_topic_value = topic_value\n",
    "            highest_coherence_topic = topic_num\n",
    "\n",
    "# Display the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f'Topic {idx}: {topic}')\n",
    "\n",
    "# Display Topics and Cluster from a specific user story\n",
    "for i, story in enumerate(user_stories):\n",
    "    print(f'\\nUser Story: {story[:100]}...')  # Display the first 100 characters for context\n",
    "    print(f'Assigned Common Topic: {highest_coherence_topic}')\n",
    "    print(f'Cluster: {clusters[i]}')\n",
    "\n",
    "# Display coherence\n",
    "print(f'Coherence of the LDA model: {coherence_lda:.4f}')\n",
    "\n",
    "# Display detailed topics for the user story with the highest topic probability\n",
    "for i, story in enumerate(user_stories):\n",
    "    topics = lda_model.get_document_topics(dictionary.doc2bow(preprocess(story)))\n",
    "    highest_topic = max(topics, key=lambda x: x[1])\n",
    "    print(f'\\nUser Story: {story[:100]}...')  # Display the first 100 characters for context\n",
    "    print('Topics:', topics)\n",
    "    print('Highest Topic:', highest_topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
